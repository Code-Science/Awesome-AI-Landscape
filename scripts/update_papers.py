import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import time
from tenacity import retry, stop_after_attempt, wait_exponential
import logging
import sys
import re

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('update_papers.log')
    ]
)
logger = logging.getLogger(__name__)

class PaperUpdater:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10),
        reraise=True
    )
    def fetch_papers(self):
        try:
            url = "https://huggingface.co/papers"
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            papers = []
            
            articles = soup.find_all('article')
            
            for article in articles:
                try:
                    # è·å–è®ºæ–‡æ ‡é¢˜å’Œé“¾æ¥
                    title_elem = article.find('h3').find('a')
                    title = title_elem.get_text(strip=True)
                    link = f"https://huggingface.co{title_elem['href']}"
                    
                    # è·å–æŠ•ç¥¨æ•° - ç›´æ¥æŸ¥æ‰¾åŒ…å«æŠ•ç¥¨æ•°çš„div
                    votes_count = 0
                    votes_div = article.find('div', class_='shadow-alternate')
                    if votes_div:
                        votes_text_div = votes_div.find('div', class_='leading-none')
                        if votes_text_div:
                            votes_text = votes_text_div.get_text(strip=True)
                            votes_count = 0 if votes_text == "-" else int(votes_text)
                    
                    papers.append({
                        'title': title,
                        'url': link,
                        'votes': votes_count
                    })
                    
                except Exception as e:
                    logger.error(f"Error parsing paper: {e}")
                    continue
            
            return papers
            
        except requests.exceptions.RequestException as e:
            logger.error(f"Error fetching papers: {e}")
            raise


    def update_readme(self, papers):
        """æ›´æ–° README.md æ–‡ä»¶"""
        try:
            with open('README.md', 'r', encoding='utf-8') as f:
                content = f.read()

            # å‡†å¤‡æ–°çš„è®ºæ–‡è¡¨æ ¼
            papers_md = "## ğŸ“° Daily Top Papers\n"
            papers_md += "> ğŸ”„ Auto-updated daily from Hugging Face Papers\n\n"
            papers_md += "| Paper | Votes | Category |\n"
            papers_md += "|-------|--------|-----------|"

            # æ·»åŠ è®ºæ–‡ä¿¡æ¯
            for i, paper in enumerate(papers[:3], 1):
                medal = {1: 'ğŸ†', 2: 'ğŸ¥ˆ', 3: 'ğŸ¥‰'}[i]
                papers_md += f"\n| {medal} [{paper['title']}]({paper['url']}) "
                papers_md += f"| â­ {paper['votes']} | AI |"

            # æ·»åŠ æ›´æ–°æ—¶é—´
            current_time = datetime.utcnow().strftime('%Y-%m-%d %H:%M UTC')
            papers_md += f"\n\n<sub>Last updated: {current_time}</sub>"

            # æ›´æ–°æ–‡ä»¶å†…å®¹
            new_content = self._replace_section(content, papers_md)
            
            with open('README.md', 'w', encoding='utf-8') as f:
                f.write(new_content)

            logger.info("README.md updated successfully")
            
        except Exception as e:
            logger.error(f"Error updating README: {e}")
            raise

    def _replace_section(self, content, new_section):
        """æ›¿æ¢ç‰¹å®šéƒ¨åˆ†çš„å†…å®¹"""
        try:
            start_marker = "## ğŸ“° Daily Top Papers"
            end_marker = "<sub>Last updated:"
            
            start_idx = content.find(start_marker)
            if start_idx == -1:
                # å¦‚æœæ‰¾ä¸åˆ°æ ‡è®°ï¼Œè¿½åŠ åˆ°æ–‡ä»¶æœ«å°¾
                return content + "\n\n" + new_section
                
            end_idx = content.find(end_marker)
            if end_idx == -1:
                end_idx = content.find("\n## ", start_idx + 1)
                
            if end_idx == -1:
                end_idx = len(content)
                
            return content[:start_idx] + new_section + content[end_idx:]
            
        except Exception as e:
            logger.error(f"Error replacing section: {e}")
            raise

    def run(self):
        """ä¸»è¿è¡Œå‡½æ•°"""
        try:
            logger.info("Starting paper update process")
            
            # è·å–è®ºæ–‡æ•°æ®
            papers = self.fetch_papers()
            
            # æŒ‰æŠ•ç¥¨æ•°æ’åº
            papers.sort(key=lambda x: x['votes'], reverse=True)
            
            # æ›´æ–° README
            self.update_readme(papers)
            
            logger.info("Paper update process completed successfully")
            
        except Exception as e:
            logger.error(f"Fatal error in main process: {e}")
            raise

if __name__ == "__main__":
    try:
        updater = PaperUpdater()
        updater.run()
    except Exception as e:
        logger.critical(f"Program terminated with error: {e}")
        sys.exit(1)
