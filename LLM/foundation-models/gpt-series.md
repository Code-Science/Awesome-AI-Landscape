# GPT Series Models

## GPT-1
- **Paper**: [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
- **Release Date**: June 2018
- **Architecture**: Transformer decoder-only
- **Parameters**: 117M
- **Training Data**: BookCorpus (4.6GB text)

## GPT-2
- **Paper**: [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- **Release Date**: February 2019
- **Parameters**: 1.5B (largest variant)
- **Model Variants**: 
  - Small (117M)
  - Medium (345M)
  - Large (762M)
  - XL (1.5B)
- **HuggingFace**: [openai-gpt2](https://huggingface.co/gpt2)

## GPT-3
- **Paper**: [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)
- **Release Date**: June 2020
- **Parameters**: 175B
- **Model Variants**:
  - Ada
  - Babbage
  - Curie
  - Davinci
- **Training Data**: ~570GB of text including Common Crawl, WebText, Books, and Wikipedia

## GPT-4
- **Release Date**: March 2023
- **Key Features**:
  - Multimodal capabilities
  - Enhanced reasoning
  - Improved safety
  - More reliable performance
- **Access**: Available through OpenAI API
- **Technical Paper**: [GPT-4 Technical Report](https://arxiv.org/abs/2303.08774)

## Notable Achievements & Impact
- Demonstrated scaling laws in language models
- Introduced few-shot learning capabilities
- Set new benchmarks in natural language processing
- Sparked discussions about AI safety and ethics

## References
- OpenAI Blog: [https://openai.com/blog](https://openai.com/blog)
- Papers With Code: [GPT Models](https://paperswithcode.com/method/gpt)
